Using TensorFlow backend.
2018-02-17 21:19:22.315112: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
Slices loaded
(84, 100, 14520)
17.1230754852
-19.9069252014
nan check
0
(6615, 84, 100)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 128, 1)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 128, 4)       40        
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 128, 128, 4)       148       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 64, 4)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 64, 64, 4)         148       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 64, 64, 4)         148       
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 32, 4)         0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 32, 32, 4)         148       
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 32, 32, 4)         148       
_________________________________________________________________
up_sampling2d_1 (UpSampling2 (None, 64, 64, 4)         0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 64, 64, 4)         148       
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 64, 64, 4)         148       
_________________________________________________________________
up_sampling2d_2 (UpSampling2 (None, 128, 128, 4)       0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 128, 128, 1)       37        
=================================================================
Total params: 1,113
Trainable params: 1,113
Non-trainable params: 0
_________________________________________________________________
Train on 4630 samples, validate on 1985 samples
Epoch 1/2

 128/4630 [..............................] - ETA: 35s - loss: 0.7043
 256/4630 [>.............................] - ETA: 30s - loss: 0.7031
 384/4630 [=>............................] - ETA: 28s - loss: 0.7024
 512/4630 [==>...........................] - ETA: 26s - loss: 0.7017
 640/4630 [===>..........................] - ETA: 25s - loss: 0.7009
 768/4630 [===>..........................] - ETA: 23s - loss: 0.7003
 896/4630 [====>.........................] - ETA: 22s - loss: 0.6997
1024/4630 [=====>........................] - ETA: 22s - loss: 0.6992
1152/4630 [======>.......................] - ETA: 21s - loss: 0.6987
1280/4630 [=======>......................] - ETA: 20s - loss: 0.6982
1408/4630 [========>.....................] - ETA: 19s - loss: 0.6978
1536/4630 [========>.....................] - ETA: 18s - loss: 0.6974
1664/4630 [=========>....................] - ETA: 17s - loss: 0.6970
1792/4630 [==========>...................] - ETA: 17s - loss: 0.6967
1920/4630 [===========>..................] - ETA: 16s - loss: 0.6964
2048/4630 [============>.................] - ETA: 15s - loss: 0.6961
2176/4630 [=============>................] - ETA: 14s - loss: 0.6958
2304/4630 [=============>................] - ETA: 14s - loss: 0.6955
2432/4630 [==============>...............] - ETA: 13s - loss: 0.6952
2560/4630 [===============>..............] - ETA: 12s - loss: 0.6950
2688/4630 [================>.............] - ETA: 11s - loss: 0.6947
2816/4630 [=================>............] - ETA: 11s - loss: 0.6945
2944/4630 [==================>...........] - ETA: 10s - loss: 0.6943
3072/4630 [==================>...........] - ETA: 9s - loss: 0.6941 
3200/4630 [===================>..........] - ETA: 8s - loss: 0.6939
3328/4630 [====================>.........] - ETA: 7s - loss: 0.6937
3456/4630 [=====================>........] - ETA: 7s - loss: 0.6935
3584/4630 [======================>.......] - ETA: 6s - loss: 0.6933
3712/4630 [=======================>......] - ETA: 5s - loss: 0.6931
3840/4630 [=======================>......] - ETA: 4s - loss: 0.6929
3968/4630 [========================>.....] - ETA: 4s - loss: 0.6927
4096/4630 [=========================>....] - ETA: 3s - loss: 0.6925
4224/4630 [==========================>...] - ETA: 2s - loss: 0.6924
4352/4630 [===========================>..] - ETA: 1s - loss: 0.6922
4480/4630 [============================>.] - ETA: 0s - loss: 0.6920
4608/4630 [============================>.] - ETA: 0s - loss: 0.6918
4630/4630 [==============================] - 32s 7ms/step - loss: 0.6918 - val_loss: 0.6852
Epoch 2/2

 128/4630 [..............................] - ETA: 25s - loss: 0.6852
 256/4630 [>.............................] - ETA: 26s - loss: 0.6851
 384/4630 [=>............................] - ETA: 25s - loss: 0.6850
 512/4630 [==>...........................] - ETA: 25s - loss: 0.6847
 640/4630 [===>..........................] - ETA: 24s - loss: 0.6846
 768/4630 [===>..........................] - ETA: 23s - loss: 0.6843
 896/4630 [====>.........................] - ETA: 22s - loss: 0.6841
1024/4630 [=====>........................] - ETA: 22s - loss: 0.6840
1152/4630 [======>.......................] - ETA: 21s - loss: 0.6838
1280/4630 [=======>......................] - ETA: 20s - loss: 0.6836
1408/4630 [========>.....................] - ETA: 19s - loss: 0.6834
1536/4630 [========>.....................] - ETA: 18s - loss: 0.6832
1664/4630 [=========>....................] - ETA: 18s - loss: 0.6830
1792/4630 [==========>...................] - ETA: 17s - loss: 0.6827
1920/4630 [===========>..................] - ETA: 16s - loss: 0.6825
2048/4630 [============>.................] - ETA: 15s - loss: 0.6822
2176/4630 [=============>................] - ETA: 14s - loss: 0.6819
2304/4630 [=============>................] - ETA: 14s - loss: 0.6816
2432/4630 [==============>...............] - ETA: 13s - loss: 0.6813
2560/4630 [===============>..............] - ETA: 12s - loss: 0.6810
2688/4630 [================>.............] - ETA: 11s - loss: 0.6807
2816/4630 [=================>............] - ETA: 10s - loss: 0.6803
2944/4630 [==================>...........] - ETA: 10s - loss: 0.6800
3072/4630 [==================>...........] - ETA: 9s - loss: 0.6797 
3200/4630 [===================>..........] - ETA: 8s - loss: 0.6793
3328/4630 [====================>.........] - ETA: 7s - loss: 0.6789
3456/4630 [=====================>........] - ETA: 7s - loss: 0.6785
3584/4630 [======================>.......] - ETA: 6s - loss: 0.6781
3712/4630 [=======================>......] - ETA: 5s - loss: 0.6776
3840/4630 [=======================>......] - ETA: 4s - loss: 0.6771
3968/4630 [========================>.....] - ETA: 3s - loss: 0.6766
4096/4630 [=========================>....] - ETA: 3s - loss: 0.6761
4224/4630 [==========================>...] - ETA: 2s - loss: 0.6755
4352/4630 [===========================>..] - ETA: 1s - loss: 0.6750
4480/4630 [============================>.] - ETA: 0s - loss: 0.6743
4608/4630 [============================>.] - ETA: 0s - loss: 0.6737
4630/4630 [==============================] - 32s 7ms/step - loss: 0.6736 - val_loss: 0.6439
----- activations -----
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
----- activations -----
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
----- activations -----
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
----- activations -----
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
----- activations -----
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
----- activations -----
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
(9, 128, 128, 1)
(9, 128, 128, 4)
(9, 128, 128, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 32, 32, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 64, 64, 4)
(9, 128, 128, 4)
(9, 128, 128, 1)
elapsed time (hrs):
0.03068938805
